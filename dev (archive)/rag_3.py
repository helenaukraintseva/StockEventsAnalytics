import os
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# --- –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º ---
EMBEDDINGS_PATH = "doc_embeddings.npy"
INDEX_PATH = "faiss_index.index"
KNOWLEDGE_PATH = "knowledge_base.json"

# --- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ---
use_cuda = os.environ.get("CUDA_VISIBLE_DEVICES") is not None
device_id = 0 if use_cuda else -1

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ---
print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º SentenceTransformer...")
embedder = SentenceTransformer("all-MiniLM-L6-v2", device="cuda" if use_cuda else "cpu")

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å...")
qa_model = pipeline(
    "text2text-generation",
    model="./rut5-small",
    tokenizer="./rut5-small",
    tokenizer_kwargs={"use_fast": False},  # üî• –í–ê–ñ–ù–û: –û—Ç–∫–ª—é—á–∞–µ–º fast —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä!
    device=device_id
)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏ FAISS –∏–Ω–¥–µ–∫—Å–∞ ---
print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
with open(KNOWLEDGE_PATH, "r", encoding="utf-8") as f:
    documents = json.load(f)

print("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏–Ω–¥–µ–∫—Å...")
doc_embeddings = np.load(EMBEDDINGS_PATH)
index = faiss.read_index(INDEX_PATH)

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ ---
def answer_question(question: str, top_k: int = 3, max_ctx_len: int = 300, debug: bool = False) -> str:
    question_embedding = embedder.encode([question])
    _, indices = index.search(question_embedding, top_k)

    # –ü–æ–¥—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
    context_chunks = [documents[i][:max_ctx_len] for i in indices[0]]
    context = "\n".join(context_chunks)

    if debug:
        print("\nüîç –í–æ–ø—Ä–æ—Å:", question)
        print("üìö –ö–æ–Ω—Ç–µ–∫—Å—Ç:\n", context)
        return "(–†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞)"

    prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n–û—Ç–≤–µ—Ç:"
    result = qa_model(prompt, max_length=100, do_sample=False)[0]['generated_text']
    return result.strip()

# --- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º ---
if __name__ == "__main__":
    print("\nü§ñ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ RAG-—Å–∏—Å—Ç–µ–º—É! –ù–∞–ø–∏—à–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.\n")

    while True:
        user_input = input("–í–æ–ø—Ä–æ—Å: ").strip()
        if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
            print("\nüëã –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
            break

        response = answer_question(user_input)
        print("üß† –û—Ç–≤–µ—Ç:", response, "\n")
