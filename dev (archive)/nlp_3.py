import pandas as pd
from datasets import Dataset
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import numpy as np

# --- 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º CSV ---
df = pd.read_csv("crypto_news_total.csv")  # –∫–æ–ª–æ–Ω–∫–∏: text, felt
df.dropna(subset=["text", "felt"], inplace=True)

# --- 2. –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ (felt -> 0,1,2) ---
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["felt"])

# --- 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ HuggingFace Dataset ---
dataset = Dataset.from_pandas(df[["text", "label"]])
dataset = dataset.train_test_split(test_size=0.2)

# --- 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
model_name = "distilroberta-base"  # –ª—ë–≥–∫–∞—è –∏ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)

encoded_dataset = dataset.map(tokenize, batched=True)

# --- 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# --- 6. –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è ---
training_args = TrainingArguments(
    output_dir="./sentiment_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    logging_dir="./logs",
)

# --- 7. –ú–µ—Ç—Ä–∏–∫–∏ ---
from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="macro"),
    }

# --- 8. Trainer ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)

# --- 9. –û–±—É—á–µ–Ω–∏–µ ---
trainer.train()

# --- 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ ---
model.save_pretrained("./sentiment_model")
tokenizer.save_pretrained("./sentiment_model")
import joblib
joblib.dump(label_encoder, "label_encoder.pkl")

# --- 11. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ ---
from transformers import pipeline
classifier = pipeline("text-classification", model="./sentiment_model", tokenizer="./sentiment_model", return_all_scores=True)
label_encoder = joblib.load("label_encoder.pkl")

def predict_sentiment(text):
    result = classifier(text)[0]
    scores = {label_encoder.inverse_transform([i])[0]: round(score["score"], 3) for i, score in enumerate(result)}
    predicted_label = max(scores, key=scores.get)
    return predicted_label, scores

# --- 12. –ü—Ä–∏–º–µ—Ä ---
text = "Ethereum is showing strong performance this week and attracting investors"
label, probs = predict_sentiment(text)
print(f"\nüì∞ –¢–µ–∫—Å—Ç: {text}")
print(f"üìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {label}")
print(f"üìà –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {probs}")
